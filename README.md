# Resume Parser & Analyzer - Phase 3 Implementation

## ğŸ“‹ Project Overview

This is a comprehensive resume parsing and analysis system that satisfies all Phase 3 requirements. The system uses advanced NLP techniques, machine learning models, and provides a web interface for easy interaction.

## âœ… Phase 3 Requirements Satisfied

### Text Processing & Cleaning
- âœ… **Noise Removal**: Uses regex patterns to identify and remove irrelevant information
- âœ… **Formatting Consistency**: Handles date formats, phone numbers, emails using dateutil and regex
- âœ… **Normalization**: Implements tokenization, stemming, and lemmatization using NLTK

### Named Entity Recognition (NER)
- âœ… **SpaCy Integration**: Uses SpaCy for NER with custom model training capabilities
- âœ… **Custom NER Training**: Trains models on labeled resume data
- âœ… **Performance Evaluation**: Calculates precision, recall, and F-score metrics

### Relationship Extraction
- âœ… **Dependency Parsing**: Extracts relationships between entities using SpaCy's dependency parser
- âœ… **Multiple Techniques**: Implements various relationship extraction methods
- âœ… **Ground Truth Validation**: Validates extracted relationships against known patterns

### Database & Analytics
- âœ… **Structured Schema**: SQLite database with normalized tables for resumes, entities, relationships
- âœ… **Data Population**: Automated insertion of parsed resume data
- âœ… **SQL Queries**: Advanced querying capabilities and analytics functions

### Machine Learning
- âœ… **ML Models**: Salary prediction, job classification, and clustering models
- âœ… **Insights Generation**: Extracts patterns and makes predictions from resume data
- âœ… **Performance Metrics**: Comprehensive model evaluation and validation

## ğŸš€ Quick Start

### For Google Colab

1. **Install Dependencies**:
```python
!pip install streamlit spacy pandas numpy scikit-learn nltk python-dateutil regex PyPDF2 python-docx seaborn matplotlib plotly wordcloud textblob
!python -m spacy download en_core_web_sm
```

2. **Clone/Download Files**:
```python
# Upload all Python files to Colab or clone from repository
```

3. **Run the Application**:
```python
!streamlit run app.py --server.port 8501 --server.address 0.0.0.0
```

### For Local Development

1. **Install Dependencies**:
```bash
pip install -r requirements.txt
python -m spacy download en_core_web_sm
```

2. **Run Application**:
```bash
streamlit run app.py
```

## ğŸ“ Project Structure

```
resume-parser/
â”œâ”€â”€ app.py                 # Main Streamlit application
â”œâ”€â”€ resume_parser.py       # Core parsing logic with NLP
â”œâ”€â”€ database_manager.py    # Database operations and schema
â”œâ”€â”€ ml_models.py          # Machine learning models
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ README.md            # This file
â””â”€â”€ models/              # Saved ML models (created automatically)
```

## ğŸ”§ Core Components

### 1. ResumeParser (`resume_parser.py`)
**Satisfies Phase 3 Requirements:**
- **Text Cleaning**: `clean_and_remove_noise()` - Removes noise using regex
- **Formatting**: `handle_formatting_inconsistencies()` - Normalizes dates, phones, emails
- **Normalization**: `normalize_text()` - Tokenization, stemming, lemmatization
- **NER**: `extract_entities()` - SpaCy-based entity extraction
- **Custom NER**: `train_custom_ner_model()` - Trains custom models
- **Evaluation**: `evaluate_ner_model()` - Precision, recall, F-score
- **Relationships**: `extract_relationships()` - Dependency parsing for relationships
- **Validation**: `validate_relationships()` - Ground truth validation

### 2. DatabaseManager (`database_manager.py`)
**Satisfies Phase 3 Requirements:**
- **Schema Design**: Structured tables for resumes, entities, relationships
- **Data Population**: `insert_resume()` - Populates database with parsed data
- **SQL Queries**: `execute_custom_query()` - Advanced SQL operations
- **Analytics**: `get_analytics_data()` - Data analysis functions

### 3. MLModels (`ml_models.py`)
**Satisfies Phase 3 Requirements:**
- **ML Training**: `train_models()` - Trains multiple ML models
- **Salary Prediction**: Random Forest regression model
- **Job Classification**: Text classification using TF-IDF and Random Forest
- **Clustering**: K-means clustering for resume segmentation
- **Insights**: `generate_insights()` - Extracts patterns and predictions

### 4. Streamlit App (`app.py`)
**Features:**
- File upload (PDF, DOCX, TXT)
- Real-time parsing and analysis
- Interactive database viewer
- Analytics dashboard with visualizations
- ML predictions interface
- Model performance metrics

## ğŸ“Š Features Breakdown

### Text Processing Pipeline
1. **File Extraction** â†’ PDF/DOCX/TXT to text
2. **Noise Removal** â†’ Regex-based cleaning
3. **Format Normalization** â†’ Consistent date/phone/email formats
4. **Text Normalization** â†’ Tokenization, stemming, lemmatization

### NLP & NER Pipeline
1. **Entity Extraction** â†’ Person, organization, skill, experience entities
2. **Custom Training** â†’ Trains NER models on labeled data
3. **Performance Evaluation** â†’ Precision, recall, F-score metrics
4. **Relationship Extraction** â†’ Subject-verb-object relationships using dependency parsing

### Database Operations
1. **Structured Storage** â†’ Normalized database schema
2. **Data Population** â†’ Automated insertion of parsed data
3. **Query Interface** â†’ Custom SQL query execution
4. **Analytics Functions** â†’ Built-in data analysis capabilities

### Machine Learning
1. **Salary Prediction** â†’ Predicts salary based on skills and experience
2. **Job Classification** â†’ Classifies job roles from resume text
3. **Resume Clustering** â†’ Groups similar resumes together
4. **Performance Metrics** â†’ Model evaluation and validation

## ğŸ¯ Usage Examples

### Upload and Parse Resume
1. Navigate to "Upload & Parse" page
2. Upload PDF/DOCX/TXT resume files
3. View parsing results including:
   - Cleaned text comparison
   - Extracted entities (NER results)
   - Relationship extraction results
   - Structured information (name, email, skills, etc.)

### Database Analysis
1. Navigate to "Database View" page
2. View all parsed resumes
3. Execute custom SQL queries
4. Analyze resume statistics

### ML Predictions
1. Navigate to "ML Predictions" page
2. Get salary predictions based on experience and skills
3. Classify job roles from resume text
4. View clustering analysis results

### Performance Monitoring
1. Navigate to "Model Performance" page
2. View NER model metrics (precision, recall, F-score)
3. Test NER model with custom text
4. Analyze relationship extraction performance

## ğŸš€ Deployment to Streamlit Cloud

### Via GitHub
1. Push code to GitHub repository
2. Connect to Streamlit Cloud
3. Deploy directly from repository
4. No additional configuration needed

### Requirements for Deployment
- All dependencies listed in `requirements.txt`
- No external API keys required
- Uses SQLite (no external database needed)
- Self-contained ML models

## ğŸ“ˆ Performance Metrics

The system provides comprehensive metrics for all components:

- **NER Model**: Precision, Recall, F1-Score
- **Relationship Extraction**: Accuracy, validation against ground truth
- **ML Models**: RMSE for regression, accuracy for classification
- **Database**: Query performance and storage metrics

## ğŸ” Code Quality Features

- **Comprehensive Documentation**: Every function documented with purpose
- **Error Handling**: Robust error handling throughout
- **Modular Design**: Separate modules for different functionalities
- **Type Hints**: Python type hints for better code clarity
- **Performance Optimization**: Efficient algorithms and caching

## ğŸ› ï¸ Troubleshooting

### Common Issues
1. **SpaCy Model Missing**: Run `python -m spacy download en_core_web_sm`
2. **NLTK Data Missing**: The code automatically downloads required NLTK data
3. **Memory Issues**: Use smaller batch sizes for large datasets
4. **File Upload Issues**: Ensure files are in supported formats (PDF, DOCX, TXT)

### For Colab Specific Issues
- Use `!streamlit run app.py --server.port 8501 --server.address 0.0.0.0`
- Install dependencies with `!pip install` prefix
- Upload files directly to Colab environment

## ğŸ“ Phase 3 Requirements Mapping

| Requirement | Implementation | File | Function |
|-------------|----------------|------|----------|
| Noise removal with regex | âœ… | `resume_parser.py` | `clean_and_remove_noise()` |
| Format inconsistencies handling | âœ… | `resume_parser.py` | `handle_formatting_inconsistencies()` |
| Normalization techniques | âœ… | `resume_parser.py` | `normalize_text()` |
| NER with SpaCy | âœ… | `resume_parser.py` | `extract_entities()` |
| Custom NER training | âœ… | `resume_parser.py` | `train_custom_ner_model()` |
| NER evaluation metrics | âœ… | `resume_parser.py` | `evaluate_ner_model()` |
| Relationship extraction | âœ… | `resume_parser.py` | `extract_relationships()` |
| Dependency parsing | âœ… | `resume_parser.py` | `extract_relationships()` |
| Relationship validation | âœ… | `resume_parser.py` | `validate_relationships()` |
| Database schema design | âœ… | `database_manager.py` | `init_database()` |
| Database population | âœ… | `database_manager.py` | `insert_resume()` |
| SQL queries & analysis | âœ… | `database_manager.py` | `execute_custom_query()` |
| ML models for insights | âœ… | `ml_models.py` | `train_models()` |

## ğŸ‰ Success Criteria Met

âœ… **All Phase 3 objectives completed**  
âœ… **Runs without errors on Colab**  
âœ… **Deployable to Streamlit Cloud**  
âœ… **No ngrok dependency**  
âœ… **Comprehensive documentation**  
âœ… **Step-by-step code explanation**  
âœ… **Production-ready code quality**
